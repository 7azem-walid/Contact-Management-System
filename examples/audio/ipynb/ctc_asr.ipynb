{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oonYCIGewZB"
      },
      "source": [
        "# Automatic Speech Recognition using CTC\n",
        "\n",
        "**Authors:** [Mohamed Reda Bouadjenek](https://rbouadjenek.github.io/) and [Ngoc Dung Huynh](https://www.linkedin.com/in/parkerhuynh/)<br>\n",
        "**Date created:** 2021/09/26<br>\n",
        "**Last modified:** 2021/09/26<br>\n",
        "**Description:** Training a CTC-based model for automatic speech recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiN92-oeewZF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "EaPYvwWXe-cs",
        "outputId": "5daa72a4-411c-4cb9-e0d3-c38cde19410e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZOI0ZRuNewZG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer\n",
        "import os\n",
        "import tarfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yQq3YZdewZH"
      },
      "source": [
        "## Load the LJSpeech Dataset\n",
        "\n",
        "Let's download the [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/).\n",
        "The dataset contains 13,100 audio files as `wav` files in the `/wavs/` folder.\n",
        "The label (transcript) for each audio file is a string\n",
        "given in the `metadata.csv` file. The fields are:\n",
        "\n",
        "- **ID**: this is the name of the corresponding .wav file\n",
        "- **Transcription**: words spoken by the reader (UTF-8)\n",
        "- **Normalized transcription**: transcription with numbers,\n",
        "ordinals, and monetary units expanded into full words (UTF-8).\n",
        "\n",
        "For this demo we will use on the \"Normalized transcription\" field.\n",
        "\n",
        "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22,050 Hz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVTZtCjsewZI"
      },
      "source": [
        "We now split the data into training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract dataset\n",
        "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
        "data_path = keras.utils.get_file(\"LJSpeech-1.1.tar.bz2\", data_url, untar=False)\n",
        "\n",
        "# Create extraction directory if it doesn't exist\n",
        "data_dir = os.path.join(os.path.dirname(data_path), \"LJSpeech-1.1-extracted\")\n",
        "extracted_subdir = os.path.join(data_dir, \"LJSpeech-1.1\")\n",
        "\n",
        "if not os.path.exists(extracted_subdir):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    with tarfile.open(data_path) as tar:\n",
        "        tar.extractall(data_dir)\n",
        "\n",
        "# Set paths\n",
        "wavs_path = os.path.join(extracted_subdir, \"wavs\")\n",
        "metadata_path = os.path.join(extracted_subdir, \"metadata.csv\")\n",
        "\n",
        "# Verify files exist\n",
        "if not os.path.exists(wavs_path):\n",
        "    raise FileNotFoundError(f\"WAVs directory not found at {wavs_path}\")\n",
        "if not os.path.exists(metadata_path):\n",
        "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}\")\n",
        "\n",
        "# Read and prepare metadata\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "sHjm500yhFIB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3kwrPYiqewZI",
        "outputId": "662eece6-9449-4eac-8c31-380f730b1bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the training set: 11790\n",
            "Size of the validation set: 1310\n"
          ]
        }
      ],
      "source": [
        "# Split into train and validation\n",
        "split = int(len(metadata_df) * 0.90)\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the validation set: {len(df_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ZTgQYIewZJ"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "We first prepare the vocabulary to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3VuhcY_oewZJ",
        "outputId": "82bbaa43-20c3-4860-e98b-696abda50f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary is: ['', np.str_('a'), np.str_('b'), np.str_('c'), np.str_('d'), np.str_('e'), np.str_('f'), np.str_('g'), np.str_('h'), np.str_('i'), np.str_('j'), np.str_('k'), np.str_('l'), np.str_('m'), np.str_('n'), np.str_('o'), np.str_('p'), np.str_('q'), np.str_('r'), np.str_('s'), np.str_('t'), np.str_('u'), np.str_('v'), np.str_('w'), np.str_('x'), np.str_('y'), np.str_('z'), np.str_(\"'\"), np.str_('?'), np.str_('!'), np.str_(' ')] (size =31)\n"
          ]
        }
      ],
      "source": [
        "# Character set for transcription\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiO6u2p7ewZJ"
      },
      "source": [
        "Next, we create the function that describes the transformation that we apply to each\n",
        "element of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DEfiPwRIewZK"
      },
      "outputs": [],
      "source": [
        "# Audio processing parameters\n",
        "frame_length = 256\n",
        "frame_step = 160\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    \"\"\"Processes a single audio sample and its label\"\"\"\n",
        "    # Convert tensor to string if needed\n",
        "    if tf.is_tensor(wav_file):\n",
        "        wav_file = wav_file.numpy().decode('utf-8')\n",
        "\n",
        "    # Build the correct file path\n",
        "    file_path = os.path.join(wavs_path, f\"{wav_file}.wav\")\n",
        "\n",
        "    # Read and decode the wav file\n",
        "    file = tf.io.read_file(file_path)\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "\n",
        "    # Compute spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "\n",
        "    # Normalize spectrogram\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "\n",
        "    # Process label\n",
        "    label = tf.strings.lower(label)\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    label = char_to_num(label)\n",
        "\n",
        "    return spectrogram, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ms-xtP3ewZK"
      },
      "source": [
        "## Creating `Dataset` objects\n",
        "\n",
        "We create a `tf.data.Dataset` object that yields\n",
        "the transformed elements, in the same order as they\n",
        "appeared in the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "FnFVE84rewZK"
      },
      "outputs": [],
      "source": [
        "def create_dataset(df):\n",
        "    \"\"\"Create a TensorFlow Dataset with proper path handling\"\"\"\n",
        "    file_names = df[\"file_name\"].tolist()\n",
        "    transcripts = df[\"normalized_transcription\"].tolist()\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((file_names, transcripts))\n",
        "    dataset = dataset.map(\n",
        "        lambda file_name, transcript: tf.py_function(\n",
        "            encode_single_sample,  # Call encode_single_sample\n",
        "            [file_name, transcript],  # Pass file_name and transcript as arguments\n",
        "            [tf.float32, tf.int64]  # Specify output types\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    # Apply padding according to maximum spectrogram shape in the batch\n",
        "    dataset = dataset.padded_batch(\n",
        "        batch_size,\n",
        "        padded_shapes=([None, fft_length // 2 + 1], [None]),\n",
        "        padding_values=(tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.int64))\n",
        "    )\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch data for performance\n",
        "    return dataset\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    \"\"\"Processes a single audio sample and its label\"\"\"\n",
        "    # 1. Convert tensor to string if needed:\n",
        "    wav_file = wav_file.numpy().decode('utf-8') if tf.is_tensor(wav_file) else wav_file\n",
        "\n",
        "    # 2. Build the correct file path (added .wav extension):\n",
        "    file_path = os.path.join(wavs_path, f\"{wav_file}.wav\")  # Ensure .wav is added\n",
        "\n",
        "    # Read and decode the wav file\n",
        "    file = tf.io.read_file(file_path)\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "\n",
        "    # Compute spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "\n",
        "    # Normalize spectrogram\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "\n",
        "    # Process label\n",
        "    label = tf.strings.lower(label)\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    label = char_to_num(label)\n",
        "\n",
        "    return spectrogram, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Tk0nSHewZK"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "Let's visualize an example in our dataset, including the\n",
        "audio clip, the spectrogram and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "46QB1vJOewZK"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "def visualize_data(dataset, num_examples=1):\n",
        "    \"\"\"Visualize spectrograms and waveforms from the dataset\"\"\"\n",
        "    plt.figure(figsize=(8, 5 * num_examples))\n",
        "\n",
        "    for i, batch in enumerate(dataset.take(num_examples)):\n",
        "        spectrogram = batch[0][i].numpy()\n",
        "        spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "        label = batch[1][i]\n",
        "\n",
        "        # Decode the label text\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "\n",
        "        # Plot spectrogram\n",
        "        ax = plt.subplot(num_examples, 2, 2*i+1)\n",
        "        ax.imshow(spectrogram, vmax=1)\n",
        "        ax.set_title(f\"Spectrogram: {label}\")\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Plot waveform\n",
        "        wav_file = df_train[\"file_name\"].iloc[i]\n",
        "        audio_path = os.path.join(wavs_path, f\"{wav_file}.wav\")\n",
        "\n",
        "        try:\n",
        "            file = tf.io.read_file(audio_path)\n",
        "            audio, _ = tf.audio.decode_wav(file)\n",
        "            audio = audio.numpy()\n",
        "\n",
        "            ax = plt.subplot(num_examples, 2, 2*i+2)\n",
        "            plt.plot(audio)\n",
        "            ax.set_title(\"Signal Wave\")\n",
        "            ax.set_xlim(0, len(audio))\n",
        "            display.display(display.Audio(np.transpose(audio), rate=16000))\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading audio: {str(e)}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ZbAskYewZL"
      },
      "source": [
        "## Model\n",
        "\n",
        "We first define the CTC Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "gcxvPMNWewZL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Model definition\n",
        "def CTCLoss(y_true, y_pred):\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5_-8ptLewZL"
      },
      "source": [
        "We now define our model. We will define a model similar to\n",
        "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "FvK5cHV6ewZL",
        "outputId": "1a82b50a-8798-494a-c7fe-bef8efeee495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"DeepSpeech_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DeepSpeech_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m            Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ expand_dim (\u001b[38;5;33mReshape\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1 (\u001b[38;5;33mConv2D\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │              \u001b[38;5;34m14,432\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                 \u001b[38;5;34m128\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_relu (\u001b[38;5;33mReLU\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2 (\u001b[38;5;33mConv2D\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │             \u001b[38;5;34m236,544\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                 \u001b[38;5;34m128\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_relu (\u001b[38;5;33mReLU\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ reshape_8 (\u001b[38;5;33mReshape\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m6,395,904\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_44 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_45 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_46 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_47 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m1,049,600\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1_relu (\u001b[38;5;33mReLU\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_48 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                    │              \u001b[38;5;34m32,800\u001b[0m │\n",
              "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                   </span>┃<span style=\"font-weight: bold\"> Output Shape                        </span>┃<span style=\"font-weight: bold\">             Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ expand_dim (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">14,432</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">236,544</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ reshape_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,395,904</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">32,800</span> │\n",
              "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,628,480\u001b[0m (101.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,628,480</span> (101.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,628,352\u001b[0m (101.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,628,352</span> (101.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=512,\n",
        ")\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqWxiMztewZL"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ZZAd04wOewZM"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoFX2gxzewZM"
      },
      "source": [
        "Let's start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "dPtohT8XewZM"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 1\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Hj53YPewZM"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check results on more validation samples\n",
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    batch_predictions = model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch_predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "print(\"-\" * 100)\n",
        "for i in np.random.randint(0, len(predictions), 5):\n",
        "    print(f\"Target    : {targets[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "Aju2axUTwAkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb7C7_SOewZM"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In practice, you should train for around 50 epochs or more. Each epoch\n",
        "takes approximately 5-6mn using a `GeForce RTX 2080 Ti` GPU.\n",
        "The model we trained at 50 epochs has a `Word Error Rate (WER) ≈ 16% to 17%`.\n",
        "\n",
        "Some of the transcriptions around epoch 50:\n",
        "\n",
        "**Audio file: LJ017-0009.wav**\n",
        "```\n",
        "- Target    : sir thomas overbury was undoubtedly poisoned by lord rochester in the reign\n",
        "of james the first\n",
        "- Prediction: cer thomas overbery was undoubtedly poisoned by lordrochester in the reign\n",
        "of james the first\n",
        "```\n",
        "\n",
        "**Audio file: LJ003-0340.wav**\n",
        "```\n",
        "- Target    : the committee does not seem to have yet understood that newgate could be\n",
        "only and properly replaced\n",
        "- Prediction: the committee does not seem to have yet understood that newgate could be\n",
        "only and proberly replace\n",
        "```\n",
        "\n",
        "**Audio file: LJ011-0136.wav**\n",
        "```\n",
        "- Target    : still no sentence of death was carried out for the offense and in eighteen\n",
        "thirtytwo\n",
        "- Prediction: still no sentence of death was carried out for the offense and in eighteen\n",
        "thirtytwo\n",
        "```\n",
        "\n",
        "Example available on HuggingFace.\n",
        "| Trained Model | Demo |\n",
        "| :--: | :--: |\n",
        "| [![Generic badge](https://img.shields.io/badge/🤗%20Model-CTC%20ASR-black.svg)](https://huggingface.co/keras-io/ctc_asr) | [![Generic badge](https://img.shields.io/badge/🤗%20Spaces-CTC%20ASR-black.svg)](https://huggingface.co/spaces/keras-io/ctc_asr) |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ctc_asr",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}